{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of week2-NER.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "id": "ddZZWE1kOd--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recognize named entities on Twitter with LSTMs\n",
        "\n",
        "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
        "\n",
        "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
        "\n",
        "    Ian Goodfellow works for Google Brain\n",
        "\n",
        "a NER model needs to provide the following sequence of tags:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
        "\n",
        "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
        "\n",
        "### Libraries\n",
        "\n",
        "For this task you will need the following libraries:\n",
        " - [Tensorflow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
        " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
        " \n",
        "If you have never worked with Tensorflow, you would probably need to read some tutorials during your work on this assignment, e.g. [this one](https://www.tensorflow.org/tutorials/recurrent) could be a good starting point. "
      ]
    },
    {
      "metadata": {
        "id": "fcBaLW70Od_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The following cell will download all data required for this assignment into the folder `week2/data`."
      ]
    },
    {
      "metadata": {
        "id": "OEZlXVOzOgE6",
        "colab_type": "code",
        "outputId": "c6fe3c26-9ef9-4074-dc33-10ed134f1d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()  \n",
        "setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_project()\n",
        "# setup_google_colab.setup_honor()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-21 01:09:46--  https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2330 (2.3K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "\rsetup_google_colab.   0%[                    ]       0  --.-KB/s               \rsetup_google_colab. 100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-21 01:09:46 (27.7 MB/s) - ‘setup_google_colab.py’ saved [2330/2330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6gUSc_N6Od_I",
        "colab_type": "code",
        "outputId": "2ee1e3ea-ad03-41aa-9efa-5a8ccd6f9362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from common.download_utils import download_week2_resources\n",
        "\n",
        "download_week2_resources()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba9e103305d745fd97fc523ddbc85b5b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=849548), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41f5178905641c3a0125a64cee26a6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=103771), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0cd72a718d44b77a40aa57fbab99c55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=106837), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9r6FgXqIOd_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Twitter Named Entity Recognition corpus\n",
        "\n",
        "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
        "\n",
        "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
      ]
    },
    {
      "metadata": {
        "id": "q8BiQxTbOd_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            if(token.startswith('@')):\n",
        "                token = '<USR>'\n",
        "                \n",
        "            elif(token.startswith('http://') or token.startswith('https://')):\n",
        "                token = '<URL>'\n",
        "                \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWJTwt9JOd_b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now we can load three separate parts of the dataset:\n",
        " - *train* data for training the model;\n",
        " - *validation* data for evaluation and hyperparameters tuning;\n",
        " - *test* data for final evaluation of the model."
      ]
    },
    {
      "metadata": {
        "id": "s9sFLuQFOd_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sR5ofqRxOd_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "OwG9W7aTOd_j",
        "colab_type": "code",
        "outputId": "81a2d025-b59a-49ee-c0ff-c63f8baf2f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1169
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VT407WwXOd_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings: \n",
        "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
        "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "\n",
        "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
      ]
    },
    {
      "metadata": {
        "id": "gklzOC1UOd_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egH_-IiSOd_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens to indices and vice versa\n",
        "    # Add special tokens to dictionaries\n",
        "    # The first special token must have index 0\n",
        "    \n",
        "    idx = 0\n",
        "    for token in special_tokens:\n",
        "        tok2idx[token] = idx\n",
        "        idx2tok.append(token)\n",
        "        idx += 1\n",
        "\n",
        "    for token_list in tokens_or_tags:\n",
        "        for token in token_list:\n",
        "            if token not in tok2idx.keys():\n",
        "                tok2idx[token] = idx\n",
        "                idx2tok.append(token)\n",
        "                idx += 1\n",
        "    \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLY49YVqOd_w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
        " - `<UNK>` token for out of vocabulary tokens;\n",
        " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
      ]
    },
    {
      "metadata": {
        "id": "UUJTFXPqOd_y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfDbGv8KOd_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
      ]
    },
    {
      "metadata": {
        "id": "O3AnpnkVOd_4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRhVS_aTOeAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a recurrent neural network\n",
        "\n",
        "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
      ]
    },
    {
      "metadata": {
        "id": "TpfmGSl_OeAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from evaluation import precision_recall_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INkMUEVGPRN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTMModel():\n",
        "    def __init__(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "        self.create_placeholders()\n",
        "        self.build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags);\n",
        "        self.compute_predictions()\n",
        "        self.compute_loss(n_tags, PAD_index)\n",
        "        self.create_optimizer()\n",
        "        \n",
        "    def create_placeholders(self):\n",
        "        \n",
        "        # Placeholders for input and ground truth output.\n",
        "        self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "        self.ground_truth_tags = self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
        "\n",
        "        # Placeholder for lengths of the sequences.\n",
        "        self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "\n",
        "        # Placeholder for a dropout keep probability. If we don't feed\n",
        "        # a value for this placeholder, it will be equal to 1.0.\n",
        "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "\n",
        "        # Placeholder for a learning rate (tf.float32).\n",
        "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')\n",
        "    \n",
        "    def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "        # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "        initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "        embedding_matrix_variable = tf.Variable(initial_embedding_matrix, name='embeddings_matrix', dtype=tf.float32)\n",
        "        embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "  \n",
        "\n",
        "        forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "            tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
        "            input_keep_prob=self.dropout_ph,\n",
        "            output_keep_prob=self.dropout_ph,\n",
        "            state_keep_prob=self.dropout_ph\n",
        "        )\n",
        "        backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "            tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
        "            input_keep_prob=self.dropout_ph,\n",
        "            output_keep_prob=self.dropout_ph,\n",
        "            state_keep_prob=self.dropout_ph\n",
        "        )\n",
        "        \n",
        "        (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "        cell_fw= forward_cell, cell_bw= backward_cell,\n",
        "        dtype=tf.float32,\n",
        "        inputs=embeddings,\n",
        "        sequence_length=self.lengths\n",
        "        )\n",
        "        rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "        self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
        "    \n",
        "    def compute_loss(self, n_tags, PAD_index):\n",
        "        ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels = ground_truth_tags_one_hot,logits = self.logits)\n",
        "\n",
        "        # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "        mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
        "        loss_tensor = tf.contrib.seq2seq.sequence_loss(logits=self.logits,targets=self.ground_truth_tags,weights=mask)\n",
        "        self.loss =  tf.reduce_mean(loss_tensor)\n",
        "        \n",
        "    def create_optimizer(self):\n",
        "        # Create an optimizer (tf.train.AdamOptimizer)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
        "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "\n",
        "        # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "        # Pay attention that you need to apply this operation only for gradients \n",
        "        # because self.grads_and_vars contains also variables.\n",
        "        # list comprehension might be useful in this case.\n",
        "        clip_norm = tf.cast(1.0, tf.float32)\n",
        "        self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
        "\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "    \n",
        "    def compute_predictions(self):\n",
        "        # Create softmax (tf.nn.softmax) function\n",
        "        softmax_output = tf.nn.softmax(self.logits)\n",
        "\n",
        "        # Use argmax (tf.argmax) to get the most probable tags\n",
        "        # Don't forget to set axis=-1\n",
        "        # otherwise argmax will be calculated in a wrong way\n",
        "        self.predictions = tf.argmax(softmax_output, axis=-1)\n",
        "    \n",
        "    def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "        feed_dict = {self.input_batch: x_batch,\n",
        "                     self.ground_truth_tags: y_batch,\n",
        "                     self.learning_rate_ph: learning_rate,\n",
        "                     self.dropout_ph: dropout_keep_probability,\n",
        "                     self.lengths: lengths}\n",
        "\n",
        "        _, loss = session.run((self.train_op, self.loss), feed_dict=feed_dict)\n",
        "        return loss\n",
        "    \n",
        "    def predict_for_batch(self, session, x_batch, lengths):\n",
        "        predictions = session.run(self.predictions, feed_dict={self.input_batch:x_batch, self.lengths:lengths})\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JjbFgcrPWbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_generator(batch_size, tokens, tags, shuffle=True, allow_small_last_batch=True):\n",
        "    num_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        sample_idxs = np.random.permutation(num_samples)\n",
        "    else:\n",
        "        sample_idxs = np.arange(num_samples)\n",
        "        \n",
        "    num_batches = num_samples//batch_size\n",
        "    \n",
        "    if allow_small_last_batch and num_samples%batch_size:\n",
        "        num_batches += 1\n",
        "    \n",
        "    for k in range(num_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k+1) * batch_size, num_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        \n",
        "        for idx in sample_idxs[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "        \n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            l = len(x_list[n])\n",
        "            x[n, :l] = x_list[n]\n",
        "            y[n, :l] = y_list[n]\n",
        "            lengths[n] = l\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjAPVmiYPa7i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_tags (model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batch_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4YIOjubZPfNA",
        "colab_type": "code",
        "outputId": "e0d226d0-d632-4c62-cc73-3aab7248818f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "model = BiLSTMModel(20505, 21, 200, 200, token2idx['<PAD>'])\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 4\n",
        "learning_rate = 0.005\n",
        "learning_rate_decay = 1.414\n",
        "dropout_keep_prob = 0.5\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch, lengths in batch_generator(batch_size, train_tokens, train_tags):\n",
        "        \n",
        "        step += 1\n",
        "        loss = model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_prob)\n",
        "\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(num_epochs) + '-' * 20)\n",
        "    print('Training loss: ', loss)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    \n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    learning_rate = learning_rate/learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 1 of 4 --------------------\n",
            "Training loss:  0.647101\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 2494 phrases; correct: 596.\n",
            "\n",
            "precision:  23.90%; recall:  13.28%; F1:  17.07\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 203 phrases; correct: 47.\n",
            "\n",
            "precision:  23.15%; recall:  8.75%; F1:  12.70\n",
            "\n",
            "-------------------- Epoch 2 of 4 --------------------\n",
            "Training loss:  0.18938679\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4745 phrases; correct: 1910.\n",
            "\n",
            "precision:  40.25%; recall:  42.55%; F1:  41.37\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 343 phrases; correct: 138.\n",
            "\n",
            "precision:  40.23%; recall:  25.70%; F1:  31.36\n",
            "\n",
            "-------------------- Epoch 3 of 4 --------------------\n",
            "Training loss:  0.104392774\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4935 phrases; correct: 2864.\n",
            "\n",
            "precision:  58.03%; recall:  63.80%; F1:  60.78\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 442 phrases; correct: 163.\n",
            "\n",
            "precision:  36.88%; recall:  30.35%; F1:  33.30\n",
            "\n",
            "-------------------- Epoch 4 of 4 --------------------\n",
            "Training loss:  0.1492335\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4653 phrases; correct: 3267.\n",
            "\n",
            "precision:  70.21%; recall:  72.78%; F1:  71.47\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 373 phrases; correct: 172.\n",
            "\n",
            "precision:  46.11%; recall:  32.03%; F1:  37.80\n",
            "\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hNj8h8XdOeAh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To compute the actual predictions of the neural network, you need to apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
      ]
    },
    {
      "metadata": {
        "id": "Sv-rMFRhOeB3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let us see full quality reports for the final model on train, validation, and test sets. To give you a hint whether you have implemented everything correctly, you might expect F-score about 40% on the validation set.\n",
        "\n",
        "**The output of the cell below (as well as the output of all the other cells) should be present in the notebook for peer2peer review!**"
      ]
    },
    {
      "metadata": {
        "id": "Q-onbNE1OeB4",
        "colab_type": "code",
        "outputId": "5a7ba184-eece-4588-8e41-eb4bfeddc90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1367
        }
      },
      "cell_type": "code",
      "source": [
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4653 phrases; correct: 3267.\n",
            "\n",
            "precision:  70.21%; recall:  72.78%; F1:  71.47\n",
            "\n",
            "\t     company: precision:   79.18%; recall:   81.03%; F1:   80.09; predicted:   658\n",
            "\n",
            "\t    facility: precision:   60.62%; recall:   68.15%; F1:   64.17; predicted:   353\n",
            "\n",
            "\t     geo-loc: precision:   77.68%; recall:   91.57%; F1:   84.06; predicted:  1174\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   34.88%; recall:    6.47%; F1:   10.91; predicted:    43\n",
            "\n",
            "\t       other: precision:   61.72%; recall:   76.88%; F1:   68.47; predicted:   943\n",
            "\n",
            "\t      person: precision:   72.57%; recall:   92.89%; F1:   81.49; predicted:  1134\n",
            "\n",
            "\t     product: precision:   53.42%; recall:   51.57%; F1:   52.48; predicted:   307\n",
            "\n",
            "\t  sportsteam: precision:   90.00%; recall:   16.59%; F1:   28.02; predicted:    40\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 373 phrases; correct: 172.\n",
            "\n",
            "precision:  46.11%; recall:  32.03%; F1:  37.80\n",
            "\n",
            "\t     company: precision:   69.33%; recall:   50.00%; F1:   58.10; predicted:    75\n",
            "\n",
            "\t    facility: precision:   29.03%; recall:   26.47%; F1:   27.69; predicted:    31\n",
            "\n",
            "\t     geo-loc: precision:   64.44%; recall:   51.33%; F1:   57.14; predicted:    90\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t       other: precision:   26.74%; recall:   28.40%; F1:   27.54; predicted:    86\n",
            "\n",
            "\t      person: precision:   41.54%; recall:   24.11%; F1:   30.51; predicted:    65\n",
            "\n",
            "\t     product: precision:   11.54%; recall:    8.82%; F1:   10.00; predicted:    26\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 463 phrases; correct: 210.\n",
            "\n",
            "precision:  45.36%; recall:  34.77%; F1:  39.36\n",
            "\n",
            "\t     company: precision:   69.57%; recall:   38.10%; F1:   49.23; predicted:    46\n",
            "\n",
            "\t    facility: precision:   42.11%; recall:   34.04%; F1:   37.65; predicted:    38\n",
            "\n",
            "\t     geo-loc: precision:   66.14%; recall:   50.91%; F1:   57.53; predicted:   127\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t musicartist: precision:  100.00%; recall:    3.70%; F1:    7.14; predicted:     1\n",
            "\n",
            "\t       other: precision:   22.90%; recall:   29.13%; F1:   25.64; predicted:   131\n",
            "\n",
            "\t      person: precision:   52.33%; recall:   43.27%; F1:   47.37; predicted:    86\n",
            "\n",
            "\t     product: precision:    6.06%; recall:    7.14%; F1:    6.56; predicted:    33\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KumSH06FOeB-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "\n",
        "Could we say that our model is state of the art and the results are acceptable for the task? Definately, we can say so. Nowadays, Bi-LSTM is one of the state of the art approaches for solving NER problem and it outperforms other classical methods. Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definately small. However, the implemented model outperforms classical CRFs for this task. Even better results could be obtained by some combinations of several types of methods, e.g. see [this](https://arxiv.org/abs/1603.01354) paper if you are interested."
      ]
    }
  ]
}